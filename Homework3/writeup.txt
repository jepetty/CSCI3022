Jessica Petty
CSCI 3022
Homework 3
October 13, 2016

APPROACH:
The first thing I did was start in the file ttest.py because that seemed pretty simple (which it was). I did the degrees_of_freedom method, with was easy given the formula provided in class. The only tricky part was that the inputs were the variance, and thus didn't need to be squared. Then I moved on to unbiased_sample_variance, which was also simple with the formula (sum of the squares of the mean subtracted from the observed values, all divided by one more than the number of elements). After this I moved onto t_statistic, where I just divided the difference of the two means by the square root of the sum of the two variances divided by the size of the samples. Finally, I did t_test by calculating the means and variances for both distributions and the t statistic of the two samples (with the method I just wrote). The trickiest part was actually learning enough about the scipy.stats.t library to calculate the p-value because it requireds the t statistic and thne the degrees of freedom plus one. After that, the p-value is twice this number subtracted from one. All of the methods in this file were quite trivial to produce given the formulas.

After this I moved on to bigrams.py. First I did chisquare_pvalue. I tried using chisquare from scipy.stats, but it didn't work quite right, so I calculated the t statistic by summing the weighted errors and then using chi2 from scipy.stats.distribution (as we did in class) to calculate the p-value. Then I implemented add_sentence, where I just looped through all the bigrams, made sure each word was in the bigram was in the dictionary, then added the counts to a dictionary where the first word was the key and the count of that bigram was the value. Then I did valid_bigrams, which was quite simple. I looped through the dictionary of dictionaries of counts I made in add_sentence then, after checking against the conditions, returned an array of the bigrams as a tuple. Definitely the trickiest of all the functions was then observed_and_expected. Eventually though, after much trial and error, I uncovered some mistakes and came to the right solution. First, I added some counters to the class to help myself: a count of the left word, a count of the right word, and a count of the bigram. I added incrementing them to the add_sentence method, but one mistake I made that caused me a lot of pain was I had to make sure to count for all words and bigrams, not just the ones that met the specifications. From there, the expected matrix was pretty simple: P(left, right) = P(left)P(right) for two independent words, then multiplying by the total number of bigrams gave the expected count. This same logic could be applied to every part of the matrix. The observed matrix was less challenging once I got the left count and the right count dictionaries going: it was just the right count (or left count) of the word minus the total bigram count. 

BIGRAMS:
There were many, many bigrams that didn't make it into the analysis because the counts of their words weren't high enough (or were too high). Initially, I was surprised not to see things like "America" or stereotypically patriotic words in the finalized lists, but most likely the counts of those words were too high. I was also surprised that less of the accepted bigrams seemed to be specifically about the current time we're in now. Of the bigrams, I see "viet" "nam", which shouldn't be two words (obviously). I also see "franklin" "roosevelt", which seems odd that it would appear often enough. "u" "n" is also in there, which I suppose technically is not two word. Overall though, I'm not too surprised by the bigrams I see: they seem pretty typical of presidential speeches. 

RESOURCES:
http://www.statisticshowto.com/satterthwaite-formula/
http://mathworld.wolfram.com/SampleVariance.html
http://www.chem.utoronto.ca/coursenotes/analsci/stats/ttest.html
http://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html
http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.t.html